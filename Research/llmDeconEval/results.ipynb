{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of the Decontamination Algorithm:\n",
    "\n",
    "https://www.statystyczny.pl/macierz-bledow-raport-dokladnosc-czulosc-precyzja/\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+----------------+---------------------+-------------------+-----------------------------------+------------+------------+\n",
      "| Version     | Task          |   Recall (TPR) |   Specificity (TNR) |   Precision (PPV) |   Negative Predictive Value (NPV) |   Accuracy |   F1 Score |\n",
      "+=============+===============+================+=====================+===================+===================================+============+============+\n",
      "| 1.0         | ARC-Easy      |        95.5367 |             51.2035 |           80.1248 |                           84.7826 |    81.0443 |    87.1546 |\n",
      "+-------------+---------------+----------------+---------------------+-------------------+-----------------------------------+------------+------------+\n",
      "| 2.0         | ARC-Challenge |        97.0476 |             76.8317 |           89.7007 |                           92.6014 |    90.4823 |    93.2296 |\n",
      "+-------------+---------------+----------------+---------------------+-------------------+-----------------------------------+------------+------------+\n",
      "| 2.0         | ARC-Easy      |        96.2898 |             71.1019 |           94.006  |                           80.2817 |    91.8761 |    95.1342 |\n",
      "+-------------+---------------+----------------+---------------------+-------------------+-----------------------------------+------------+------------+\n",
      "| Avg-New     | Avg-New       |        96.6687 |             73.9668 |           91.8534 |                           86.4416 |    91.1792 |    94.1819 |\n",
      "+-------------+---------------+----------------+---------------------+-------------------+-----------------------------------+------------+------------+\n",
      "| Improvement | Improvement   |         1.13   |             22.76   |           11.73   |                            1.66   |    10.13   |     7.03   |\n",
      "+-------------+---------------+----------------+---------------------+-------------------+-----------------------------------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def calculate(TP, TN, FP, FN, version, task):\n",
    "    metrics = {\n",
    "        \"Version\": version,\n",
    "        \"Task\": task,\n",
    "        \"Recall (TPR)\": TP / (TP + FN) * 100,\n",
    "        \"Specificity (TNR)\": TN / (TN + FP) * 100,\n",
    "        \"Precision (PPV)\": TP / (TP + FP) * 100,\n",
    "        \"Negative Predictive Value (NPV)\": TN / (TN + FN) * 100,\n",
    "        \"Accuracy\": (TP + TN) / (TP + TN + FP + FN) * 100,\n",
    "        \"F1 Score\": 2 * (TP / (TP + FP) * 100 * TP / (TP + FN) * 100) / (TP / (TP + FP) * 100 + TP / (TP + FN) * 100)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# LLM as a Judge validation (cost with development tests)\n",
    "metrics0 = calculate(1798, 468, 446, 84, '1.0', 'ARC-Easy')      # 4.76$\n",
    "metrics1 = calculate(1019, 388, 117, 31, '2.0', 'ARC-Challenge') # 3.07$\n",
    "metrics2 = calculate(2180, 342, 139, 84, '2.0', 'ARC-Easy')      # 5.17$ \n",
    "                                   #Total cost of validation     # 13.0$ (50 PLN)\n",
    "                                   #Reqests: 8304 \n",
    "                                   #GPT-4o\n",
    "metrics_avg = {key: (metrics1[key] + metrics2[key]) / 2 if key not in ['Version', 'Task'] else 'Avg-New' for key in metrics1}\n",
    "\n",
    "metrics_diff = {key: round(metrics_avg[key] - metrics0[key], 2) if key not in ['Version', 'Task'] else 'Improvement' for key in metrics0}\n",
    "\n",
    "print(tabulate([metrics0,metrics1, metrics2, metrics_avg, metrics_diff], headers=\"keys\", tablefmt=\"grid\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
